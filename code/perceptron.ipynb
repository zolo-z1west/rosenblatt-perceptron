{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f17db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314ca01",
   "metadata": {},
   "source": [
    "the perceptron is a simple computational unit that receives input signals S, multiplies each by a connection weight w, sums them, and then compares the result to a threshold. it outputs either 1 or 0.\n",
    "rosenblatt through this paper tried to replicate a biological neuron in a simple manner.\n",
    "the excitation E of the perceptron is the weighted sum \n",
    "of its inputs:\n",
    "    E = Σ ( Sᵢ · wᵢ )\n",
    "this represents how strongly the input pattern activates \n",
    "the response unit before applying the threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def excitation(S, w):\n",
    "    E = 0\n",
    "    for i in range(len(S)):\n",
    "        E += S[i] * w[i]     #\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68555ca7",
   "metadata": {},
   "source": [
    "ok so now rosenblatt went onto to propose a response function. what the response function basically does is that it will fire (R=1) only if the exitation state exceeds a threshold theta.\n",
    "\n",
    "i.e -> \n",
    "        R=1 ; if E>0 |\n",
    "        R=0 ; if E<=0\n",
    "\n",
    "this makes the excitation into a binary decision.\n",
    "(defined as funct 'response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(E, theta):\n",
    "    if E > theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    # response function returns 1 if excitation is greater than threshold theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5dc33",
   "metadata": {},
   "source": [
    "now we implement a predict function. the predict function will basically use the above two function definitions to generate a prediction on the said data i.e the response of the neuron.\n",
    "so \n",
    "    data-->neuoron---> reponse      <--is what the neuron generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(S, w, theta):\n",
    "    E = excitation(S, w)\n",
    "    return response(E, theta)\n",
    "# predict function gives the final output of the perceptron based on inputs S, weights w and threshold theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5700ccb",
   "metadata": {},
   "source": [
    "The Learning Rule( as detailed by Rosenblatt)\n",
    "\n",
    "what we want and what we get is sometimes or usually different. if we expect T outcome and get response R, then we find ways to correct initial approach to get the original outcome. \n",
    "\n",
    "similiarly, if the perceptron fires response R when desired target is T, the correction i.e weights needs to be adjusted. so if we fire a little more of our own brain neurons, we get the following :\n",
    "\n",
    "    delta(weights) = eta*(T-R)*(summation)\n",
    "\n",
    "now we've already said what T,R and summation is. what is eta? \n",
    "how much we correct out initial approach matters more than how much we change it. eta defines how much we correct it. it is also termed as 'learning rate' in many adaptations. \n",
    "\n",
    "this correction is applied to weights by\n",
    "        weight_original. <---- initial_weight + delta(weight)\n",
    "\n",
    "this applies to each and every input and its weight the neuron gets..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, S, T, R, eta):\n",
    "    for i in range(len(w)):\n",
    "        delta_w = eta * (T - R) * S[i]   \n",
    "        w[i] = w[i] + delta_w             \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ed82f",
   "metadata": {},
   "source": [
    "The Training Algorithm - as explained by Rosenblatt\n",
    "\n",
    "Rosenblatt's training algorithm repeatedly inputs patterns and updates weights whenever perceptron makes a mistakes. so for each stimulus S, it computes excitation E and response R, compares R with original target T, and then applies the correction accordingly (as shown in the last markdown)..\n",
    "\n",
    "how do we implement this below?\n",
    "we define a function with params X(the input pattern), T(the target), eta(the learning rate), theta(the threshold as specified by rosenblatt in his response rule), and epochs(it basically means how many times we repeat the training set)\n",
    "\n",
    "The training function begins by figuring out how many features \n",
    "each input example has. If each example looks like [x1, x2], \n",
    "then the perceptron needs two weights—one for each feature. \n",
    "These weights are created as small random numbers because the \n",
    "perceptron starts with no knowledge about the problem.\n",
    "\n",
    "Next, the function enters a loop that repeats for several epochs. \n",
    "An epoch is just one full pass over the entire training dataset. \n",
    "Repeating the data helps the perceptron improve little by little.\n",
    "\n",
    "Inside each epoch, the code goes through every training pair: \n",
    "the input pattern (S) and the correct output (target). For each \n",
    "pattern, the perceptron uses the current weights to make a guess \n",
    "by calling predict(S, w, theta). This guess is either 0 or 1.\n",
    "\n",
    "If the guess matches the target, nothing needs to change. But if \n",
    "the perceptron guesses incorrectly, the update_weights function \n",
    "is called. This applies Rosenblatt’s learning rule to adjust the \n",
    "weights slightly in the direction that would make the perceptron \n",
    "more correct next time. Each mistake causes a small correction.\n",
    "\n",
    "After all epochs are finished, the perceptron has updated its \n",
    "weights many times and has learned a rule that separates the data. \n",
    "The function returns these final weights along with the threshold, \n",
    "representing the learned decision boundary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, T, eta=0.1, theta=0, epochs=10):\n",
    "    num_features = len(X[0])\n",
    "    w=[]\n",
    "    for i in range(num_features):\n",
    "        w.append(random.uniform(-1, 1))\n",
    "    for _ in range(epochs):\n",
    "        for S, target in zip(X, T):\n",
    "            R = predict(S, w, theta)\n",
    "            if R != target:\n",
    "                w = update_weights(w, S, target, R, eta)\n",
    "    return w, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70421ab9",
   "metadata": {},
   "source": [
    "this was the how the perceptron was proposed and implemented by Rosenblatt. I have used the same variables names as given in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02c53b",
   "metadata": {},
   "source": [
    "**Testing and usage of the perceptron in a dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48202e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
